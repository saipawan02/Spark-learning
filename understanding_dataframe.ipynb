{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 22:30:22 WARN Utils: Your hostname, Ds-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.29.89 instead (on interface en0)\n",
      "24/03/16 22:30:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/16 22:30:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.89:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10b50e450>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id|names|age|experience|\n",
      "+---+-----+---+----------+\n",
      "|  0|  sai| 12|         1|\n",
      "|  1|pawan| 24|         2|\n",
      "|  2|    d| 36|         3|\n",
      "+---+-----+---+----------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- names: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('test1.csv', header= True, inferSchema=True) \n",
    "#inferSchema option tells the reader to infer data types from the source file.\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|names|experience|\n",
      "+-----+----------+\n",
      "|  sai|         1|\n",
      "|pawan|         2|\n",
      "|    d|         3|\n",
      "+-----+----------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 22:30:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+----------+\n",
      "|summary| id|names| age|experience|\n",
      "+-------+---+-----+----+----------+\n",
      "|  count|  3|    3|   3|         3|\n",
      "|   mean|1.0| NULL|24.0|       2.0|\n",
      "| stddev|1.0| NULL|12.0|       1.0|\n",
      "|    min|  0|    d|  12|         1|\n",
      "|    max|  2|  sai|  36|         3|\n",
      "+-------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_name_exp = df.select(['names', 'experience'])\n",
    "df_name_exp.show()\n",
    "\n",
    "print(type(df_name_exp))\n",
    "df.dtypes\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'names'> <class 'pyspark.sql.column.Column'>\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/71/c7pdh8bj3p1chk4tv474y1g80000gn/T/ipykernel_14789/3921233689.py\", line 7, in <module>\n",
      "    names.show()\n",
      "TypeError: 'Column' object is not callable\n"
     ]
    }
   ],
   "source": [
    "# Picking up a singel column will change the datatype to column not dataframe.\n",
    "names = df_name_exp['names']\n",
    "print(names, type(names))\n",
    "\n",
    "try:\n",
    "    # Show only works on the dataframe not on columns\n",
    "    names.show()\n",
    "except TypeError as e:\n",
    "    print(traceback.print_exception(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+------------------------+\n",
      "| id|names|age|experience|Experience after 2 years|\n",
      "+---+-----+---+----------+------------------------+\n",
      "|  0|  sai| 12|         1|                       3|\n",
      "|  1|pawan| 24|         2|                       4|\n",
      "|  2|    d| 36|         3|                       5|\n",
      "+---+-----+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_clm = df.withColumn('Experience after 2 years', df['experience'] + 2)\n",
    "df_new_clm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping the Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id|names|age|experience|\n",
      "+---+-----+---+----------+\n",
      "|  0|  sai| 12|         1|\n",
      "|  1|pawan| 24|         2|\n",
      "|  2|    d| 36|         3|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_clm.drop('Experience after 2 years').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|experience|\n",
      "+---+-----+---+----------+\n",
      "|  0|  sai| 12|         1|\n",
      "|  1|pawan| 24|         2|\n",
      "|  2|    d| 36|         3|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('names', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new rows using Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+----------+\n",
      "|  id|   names| age|experience|\n",
      "+----+--------+----+----------+\n",
      "|   3|saipawan|  34|         4|\n",
      "|   4|  pawand|NULL|         5|\n",
      "|   5|    said|NULL|      NULL|\n",
      "|NULL|    NULL|NULL|      NULL|\n",
      "+----+--------+----+----------+\n",
      "\n",
      "+----+--------+----+----------+\n",
      "|  id|   names| age|experience|\n",
      "+----+--------+----+----------+\n",
      "|   0|     sai|  12|         1|\n",
      "|   1|   pawan|  24|         2|\n",
      "|   2|       d|  36|         3|\n",
      "|   3|saipawan|  34|         4|\n",
      "|   4|  pawand|NULL|         5|\n",
      "|   5|    said|NULL|      NULL|\n",
      "|NULL|    NULL|NULL|      NULL|\n",
      "+----+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "new_rows = spark.createDataFrame(\n",
    "    [\n",
    "        (3, 'saipawan', 34, 4),\n",
    "        (4, 'pawand', None, 5),\n",
    "        (5, 'said', None, None),\n",
    "        (None, None, None, None)\n",
    "    ],\n",
    "     df.columns\n",
    "    )\n",
    "new_rows.show()\n",
    "\n",
    "df = df.union(new_rows)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "|_id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|     sai|  2000|\n",
      "|  2|   pawan|  3000|\n",
      "|  3|    dsai|  4000|\n",
      "|  4|saipawan|  5000|\n",
      "|  2|   pawan|  3000|\n",
      "+---+--------+------+\n",
      "\n",
      "+---+--------+------+\n",
      "|_id|    name|salary|\n",
      "+---+--------+------+\n",
      "|  1|     sai|  2000|\n",
      "|  2|   pawan|  3000|\n",
      "|  3|    dsai|  4000|\n",
      "|  4|saipawan|  5000|\n",
      "|  2|   pawan|  3000|\n",
      "+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [[1, 'sai', 2000], [2, 'pawan', 3000]]\n",
    "schema1 = ['_id', 'name', 'salary']\n",
    "\n",
    "data2 = [[3, 'dsai', 4000], [4, 'saipawan', 5000], [2, 'pawan', 3000]]\n",
    "schema2 = ['_id', 'name', 'salary']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "\n",
    "# Unlike sql, the union function does not remove the duplicate rows\n",
    "df1.union(df2).show()\n",
    "df1.unionAll(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot resolve column name \"salary\" among (_id, name, gender).\n",
      "+---+--------+------+------+\n",
      "|_id|    name|salary|gender|\n",
      "+---+--------+------+------+\n",
      "|  1|     sai|  2000|  NULL|\n",
      "|  2|   pawan|  3000|  NULL|\n",
      "|  3|    dsai|  NULL|  male|\n",
      "|  4|saipawan|  NULL|  male|\n",
      "|  5|     xyz|  NULL|female|\n",
      "+---+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [[1, 'sai', 2000], [2, 'pawan', 3000]]\n",
    "schema1 = ['_id', 'name', 'salary']\n",
    "\n",
    "data2 = [[3, 'dsai', 'male'], [4, 'saipawan', 'male'], [5, 'xyz', 'female']]\n",
    "schema2 = ['_id', 'name', 'gender']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "\n",
    "try:\n",
    "    # This will throw an error as there is an mismatch in the col name.\n",
    "    df1.unionByName(df2)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# This will merge the dataFrames using the column name.\n",
    "df1.unionByName(df2, allowMissingColumns= True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinct and DropDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "|_id| name|gender|salary|\n",
      "+---+-----+------+------+\n",
      "|  1|  sai|  male|  2000|\n",
      "|  2|pawan|  male|  3000|\n",
      "|  2|pawan|  male|  3000|\n",
      "|  3|    d|female|  4000|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 'sai', 'male', 2000],[2, 'pawan', 'male', 3000], [2, 'pawan', 'male', 3000], [3, 'd', 'female', 4000]]\n",
    "schema = ['_id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "|_id| name|gender|salary|\n",
      "+---+-----+------+------+\n",
      "|  1|  sai|  male|  2000|\n",
      "|  2|pawan|  male|  3000|\n",
      "|  3|    d|female|  4000|\n",
      "+---+-----+------+------+\n",
      "\n",
      "+---+-----+------+------+\n",
      "|_id| name|gender|salary|\n",
      "+---+-----+------+------+\n",
      "|  1|  sai|  male|  2000|\n",
      "|  2|pawan|  male|  3000|\n",
      "|  3|    d|female|  4000|\n",
      "+---+-----+------+------+\n",
      "\n",
      "+---+----+------+------+\n",
      "|_id|name|gender|salary|\n",
      "+---+----+------+------+\n",
      "|  3|   d|female|  4000|\n",
      "|  1| sai|  male|  2000|\n",
      "+---+----+------+------+\n",
      "\n",
      "+---+-----+------+------+\n",
      "|_id| name|gender|salary|\n",
      "+---+-----+------+------+\n",
      "|  3|    d|female|  4000|\n",
      "|  1|  sai|  male|  2000|\n",
      "|  2|pawan|  male|  3000|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()\n",
    "\n",
    "df.dropDuplicates().show()\n",
    "# now it will only show the 1st occurance\n",
    "df.dropDuplicates(['gender']).show()\n",
    "df.dropDuplicates(['gender', 'salary']).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+----------+\n",
      "|  id|    name| age|experience|\n",
      "+----+--------+----+----------+\n",
      "|   0|     sai|  12|         1|\n",
      "|   1|   pawan|  24|         2|\n",
      "|   2|       d|  36|         3|\n",
      "|   3|saipawan|  34|         4|\n",
      "|   4|  pawand|NULL|         5|\n",
      "|   5|    said|NULL|      NULL|\n",
      "|NULL|    NULL|NULL|      NULL|\n",
      "+----+--------+----+----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- experience: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (0, 'sai', 12, 1),\n",
    "        (1, 'pawan', 24, 2),\n",
    "        (2, 'd', 36, 3),\n",
    "        (3, 'saipawan', 34, 4),\n",
    "        (4, 'pawand', None, 5),\n",
    "        (5, 'said', None, None),\n",
    "        (None, None, None, None)\n",
    "    ],\n",
    "     ['id', 'name','age', 'experience']\n",
    "    )\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+----------+\n",
      "| id|    name|age|experience|\n",
      "+---+--------+---+----------+\n",
      "|  0|     sai| 12|         1|\n",
      "|  1|   pawan| 24|         2|\n",
      "|  2|       d| 36|         3|\n",
      "|  3|saipawan| 34|         4|\n",
      "+---+--------+---+----------+\n",
      "\n",
      "+---+--------+----+----------+\n",
      "| id|    name| age|experience|\n",
      "+---+--------+----+----------+\n",
      "|  0|     sai|  12|         1|\n",
      "|  1|   pawan|  24|         2|\n",
      "|  2|       d|  36|         3|\n",
      "|  3|saipawan|  34|         4|\n",
      "|  4|  pawand|NULL|         5|\n",
      "|  5|    said|NULL|      NULL|\n",
      "+---+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any').show()  # this is the default values\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+----------+\n",
      "| id|    name| age|experience|\n",
      "+---+--------+----+----------+\n",
      "|  0|     sai|  12|         1|\n",
      "|  1|   pawan|  24|         2|\n",
      "|  2|       d|  36|         3|\n",
      "|  3|saipawan|  34|         4|\n",
      "|  4|  pawand|NULL|         5|\n",
      "|  5|    said|NULL|      NULL|\n",
      "+---+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any',thresh=2).show()\n",
    "# This will delete all the rows with dataset which have more than <thresh> non null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+----------+\n",
      "| id|    name| age|experience|\n",
      "+---+--------+----+----------+\n",
      "|  0|     sai|  12|         1|\n",
      "|  1|   pawan|  24|         2|\n",
      "|  2|       d|  36|         3|\n",
      "|  3|saipawan|  34|         4|\n",
      "|  4|  pawand|NULL|         5|\n",
      "+---+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any', subset=['experience']).show()\n",
    "# Specify which columns you want to focus on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+----+----------+\n",
      "|  id|          name| age|experience|\n",
      "+----+--------------+----+----------+\n",
      "|   0|           sai|  12|         1|\n",
      "|   1|         pawan|  24|         2|\n",
      "|   2|             d|  36|         3|\n",
      "|   3|      saipawan|  34|         4|\n",
      "|   4|        pawand|NULL|         5|\n",
      "|   5|          said|NULL|      NULL|\n",
      "|NULL|Missing Values|NULL|      NULL|\n",
      "+----+--------------+----+----------+\n",
      "\n",
      "+----+--------+---+----------+\n",
      "|  id|    name|age|experience|\n",
      "+----+--------+---+----------+\n",
      "|   0|     sai| 12|         1|\n",
      "|   1|   pawan| 24|         2|\n",
      "|   2|       d| 36|         3|\n",
      "|   3|saipawan| 34|         4|\n",
      "|   4|  pawand|  0|         5|\n",
      "|   5|    said|  0|         0|\n",
      "|NULL|    NULL|  0|         0|\n",
      "+----+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill and fillNa will work the same way.\n",
    "df.fillna('Missing Values', subset=['name']).show()\n",
    "\n",
    "df.na.fill( 0 , subset=['age', 'experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+----------+----------+-----------------+\n",
      "|  id|    name| age|experience|age_impute|experience_impute|\n",
      "+----+--------+----+----------+----------+-----------------+\n",
      "|   0|     sai|  12|         1|        12|                1|\n",
      "|   1|   pawan|  24|         2|        24|                2|\n",
      "|   2|       d|  36|         3|        36|                3|\n",
      "|   3|saipawan|  34|         4|        34|                4|\n",
      "|   4|  pawand|NULL|         5|        26|                5|\n",
      "|   5|    said|NULL|      NULL|        26|                3|\n",
      "|NULL|    NULL|NULL|      NULL|        26|                3|\n",
      "+----+--------+----+----------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "cols = ['age', 'experience']\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = cols,\n",
    "    outputCols = [ f'{col}_impute' for col in cols]\n",
    ").setStrategy('mean')\n",
    "\n",
    "imputer.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter/where on DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+----------+\n",
      "|  id|    name| age|experience|\n",
      "+----+--------+----+----------+\n",
      "|   0|     sai|  12|         1|\n",
      "|   1|   pawan|  24|         2|\n",
      "|   2|       d|  36|         3|\n",
      "|   3|saipawan|  34|         4|\n",
      "|   4|  pawand|NULL|         5|\n",
      "|   5|    said|NULL|      NULL|\n",
      "|NULL|    NULL|NULL|      NULL|\n",
      "+----+--------+----+----------+\n",
      "\n",
      "+---+--------+---+----------+\n",
      "| id|    name|age|experience|\n",
      "+---+--------+---+----------+\n",
      "|  1|   pawan| 24|         2|\n",
      "|  3|saipawan| 34|         4|\n",
      "+---+--------+---+----------+\n",
      "\n",
      "+---+--------+---+----------+\n",
      "| id|    name|age|experience|\n",
      "+---+--------+---+----------+\n",
      "|  1|   pawan| 24|         2|\n",
      "|  2|       d| 36|         3|\n",
      "|  3|saipawan| 34|         4|\n",
      "+---+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "\n",
    "# Both filter and where works the same.\n",
    "df.filter(\n",
    "    (df['age'] > 15) & (df['age'] < 35)\n",
    ").show()\n",
    "# other operations are &,|, ==, ~\n",
    "\n",
    "df.filter(\n",
    "    ~(df['age'] < 15)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy and Aggrigate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|    Name|Department|Salary|\n",
      "+--------+----------+------+\n",
      "|     Sai|     Maths| 10000|\n",
      "|     Sai|   English|  5000|\n",
      "|   Pawan|   Science|  4000|\n",
      "|     Sai|   Science|  4000|\n",
      "|pawansai|     Maths|  3000|\n",
      "|saipawan|     Maths| 20000|\n",
      "|saipawan|   English| 10000|\n",
      "|saipawan|   Science|  5000|\n",
      "|    dsai|     Maths| 10000|\n",
      "|    dsai|   Science|  2000|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [['Sai', 'Maths', 10000],\n",
    "['Sai', 'English', 5000],\n",
    "['Pawan', 'Science', 4000],\n",
    "['Sai', 'Science', 4000],\n",
    "['pawansai', 'Maths', 3000],\n",
    "['saipawan', 'Maths', 20000],\n",
    "['saipawan', 'English', 10000],\n",
    "['saipawan', 'Science', 5000],\n",
    "['dsai', 'Maths', 10000],\n",
    "['dsai', 'Science', 2000]]\n",
    "\n",
    "df = spark.createDataFrame(data, ['Name', 'Department', 'Salary'],)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    Name|sum(Salary)|\n",
      "+--------+-----------+\n",
      "|     Sai|      19000|\n",
      "|   Pawan|       4000|\n",
      "|pawansai|       3000|\n",
      "|saipawan|      35000|\n",
      "|    dsai|      12000|\n",
      "+--------+-----------+\n",
      "\n",
      "+----------+-----------+\n",
      "|Department|avg(Salary)|\n",
      "+----------+-----------+\n",
      "|   Science|     3750.0|\n",
      "|   English|     7500.0|\n",
      "|     Maths|    10750.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').sum().show()\n",
    "\n",
    "df.groupBy('Department').avg().sort('avg(Salary)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+-----------+-----------+\n",
      "|Department|Count of all employes|min(salary)|max(salary)|\n",
      "+----------+---------------------+-----------+-----------+\n",
      "|     Maths|                    4|       3000|      20000|\n",
      "|   English|                    2|       5000|      10000|\n",
      "|   Science|                    4|       2000|       5000|\n",
      "+----------+---------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, min, max\n",
    "\n",
    "# Agg function are used to appply multipe aggrigate operation on one grp by action.\n",
    "# * means all the columns.\n",
    "df.groupby('Department').agg(count('*').alias(\"Count of all employes\"), min('salary'), max('salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+------+\n",
      "| id|    Name|Department|Gender|\n",
      "+---+--------+----------+------+\n",
      "|  1|     Sai|     Maths|  male|\n",
      "|  2|     Sai|   English|  male|\n",
      "|  3|   Pawan|   Science|female|\n",
      "|  4|     Sai|   Science|female|\n",
      "|  5|pawansai|     Maths|female|\n",
      "|  6|saipawan|     Maths|  male|\n",
      "|  7|saipawan|   English|  male|\n",
      "|  8|saipawan|   Science|  male|\n",
      "|  9|    dsai|     Maths|female|\n",
      "| 10|    dsai|   Science|female|\n",
      "+---+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1,'Sai', 'Maths', 'male'],\n",
    "[2, 'Sai', 'English', 'male'],\n",
    "[3,'Pawan', 'Science', 'female'],\n",
    "[4, 'Sai', 'Science', 'female'],\n",
    "[5, 'pawansai', 'Maths', 'female'],\n",
    "[6, 'saipawan', 'Maths', 'male'],\n",
    "[7, 'saipawan', 'English', 'male'],\n",
    "[8, 'saipawan', 'Science', 'male'],\n",
    "[9, 'dsai', 'Maths', 'female'],\n",
    "[10, 'dsai', 'Science', 'female']]\n",
    "\n",
    "df = spark.createDataFrame(data, ['id', 'Name', 'Department', 'Gender'],)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|Department|Gender|count|\n",
      "+----------+------+-----+\n",
      "|     Maths|  male|    2|\n",
      "|   English|  male|    2|\n",
      "|   Science|female|    3|\n",
      "|     Maths|female|    2|\n",
      "|   Science|  male|    1|\n",
      "+----------+------+-----+\n",
      "\n",
      "+----------+------+----+\n",
      "|Department|female|male|\n",
      "+----------+------+----+\n",
      "|   Science|     3|   1|\n",
      "|   English|  NULL|   2|\n",
      "|     Maths|     2|   2|\n",
      "+----------+------+----+\n",
      "\n",
      "+----------+----+\n",
      "|Department|male|\n",
      "+----------+----+\n",
      "|   Science|   1|\n",
      "|   English|   2|\n",
      "|     Maths|   2|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(['Department', 'Gender']).count().show()\n",
    "\n",
    "# Pivot the gender column to get number of males and females in one department\n",
    "pivot_df = df.groupBy('Department').pivot('Gender').count()\n",
    "pivot_df.show()\n",
    "\n",
    "# If I have to dsiaply only number of males in the department.\n",
    "df.groupBy('Department').pivot('Gender', ['male']).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+\n",
      "|Department|female|male|\n",
      "+----------+------+----+\n",
      "|   Science|     3|   1|\n",
      "|   English|  NULL|   2|\n",
      "|     Maths|     2|   2|\n",
      "+----------+------+----+\n",
      "\n",
      "+----------+------+-----+\n",
      "|Department|gender|count|\n",
      "+----------+------+-----+\n",
      "|   Science|     M|    1|\n",
      "|   Science|     F|    3|\n",
      "|   English|     M|    2|\n",
      "|   English|     F| NULL|\n",
      "|     Maths|     M|    2|\n",
      "|     Maths|     F|    2|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unpivot functions\n",
    "# There is not unpivot functionality in pyspark, we have to use stack functionality to unpivot.\n",
    "\n",
    "pivot_df.show()\n",
    "\n",
    "# Now I want this above dataframe with columns as Department, Gneder, Count as below.\n",
    "\"\"\"\n",
    "|Department|Gender|count|\n",
    "+----------+------+-----+\n",
    "|     Maths|  male|    2|\n",
    "|   English|  male|    2|\n",
    "|   Science|female|    3|\n",
    "|     Maths|female|    2|\n",
    "|   Science|  male|    1|\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import expr, stack\n",
    "\n",
    "# expr fucntion takes a sting as an input but treat that as a python code.\n",
    "pivot_df.select('Department', expr(\"stack(2, 'M', male, 'F', female) as (gender, count)\")).show()\n",
    "# here 2 represents number of columns values that we are unpivoting, followed by their names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OrderBy and Sort functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+\n",
      "|_id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  1|     sai|  male|  5000|\n",
      "|  2|   pawan|  male|  2000|\n",
      "|  2|saipawan|  male| 10000|\n",
      "+---+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 'sai', 'male', 5000],[2, 'pawan', 'male', 2000], [2, 'saipawan', 'male', 10000]]\n",
    "schema = ['_id', 'name', 'gender', 'salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+\n",
      "|_id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  2|   pawan|  male|  2000|\n",
      "|  1|     sai|  male|  5000|\n",
      "|  2|saipawan|  male| 10000|\n",
      "+---+--------+------+------+\n",
      "\n",
      "+---+--------+------+------+\n",
      "|_id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  2|saipawan|  male| 10000|\n",
      "|  1|     sai|  male|  5000|\n",
      "|  2|   pawan|  male|  2000|\n",
      "+---+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.salary).show()\n",
    "df.sort(df.salary.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+------+\n",
      "|_id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  2|   pawan|  male|  2000|\n",
      "|  1|     sai|  male|  5000|\n",
      "|  2|saipawan|  male| 10000|\n",
      "+---+--------+------+------+\n",
      "\n",
      "+---+--------+------+------+\n",
      "|_id|    name|gender|salary|\n",
      "+---+--------+------+------+\n",
      "|  2|saipawan|  male| 10000|\n",
      "|  1|     sai|  male|  5000|\n",
      "|  2|   pawan|  male|  2000|\n",
      "+---+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.salary).show()\n",
    "df.orderBy(df.salary.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 24|\n",
      "| 27|\n",
      "| 28|\n",
      "| 29|\n",
      "| 45|\n",
      "| 46|\n",
      "| 50|\n",
      "| 62|\n",
      "| 68|\n",
      "| 83|\n",
      "| 86|\n",
      "| 97|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(start = 1, end = 101)\n",
    "df.show(5) # Desplaying only 1st 5 elements\n",
    "\n",
    "# This will give approx 10% of the values selected at random, sometimes it can be more than 10% and sometimes it can be less than 10%\n",
    "df1 = df.sample(fraction= 0.1) \n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| Id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  sai|  2000|\n",
      "|  2|pawan|  3000|\n",
      "+---+-----+------+\n",
      "\n",
      "Type of list element:  <class 'pyspark.sql.types.Row'>\n",
      "\n",
      "1 sai 2000\n",
      "2 pawan 3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNote: this function is not recommended on very large data, as to perform this function we need collect all the data on one node \\nand we may face out of memory error.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 'sai', 2000),\n",
    "    (2, 'pawan', 3000)\n",
    "]\n",
    "\n",
    "schema = ['Id', 'name', 'salary']\n",
    "\n",
    "df =  spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "lst_of_rows = df.collect()\n",
    "print(\"Type of list element: \",type(lst_of_rows[0]))\n",
    "print()\n",
    "\n",
    "for row in lst_of_rows:\n",
    "    print(row.Id, row.name, row.salary)\n",
    "\n",
    "'''\n",
    "Note: this function is not recommended on very large data, as to perform this function we need collect all the data on one node \n",
    "and we may face out of memory error.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfrom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------------------+\n",
      "| id| name|salary in Thousnds|\n",
      "+---+-----+------------------+\n",
      "|  1|  sai|                20|\n",
      "|  2|pawan|                30|\n",
      "+---+-----+------------------+\n",
      "\n",
      "+---+-----+---------------+\n",
      "| id| name|salary in Lakhs|\n",
      "+---+-----+---------------+\n",
      "|  1|  SAI|            2.0|\n",
      "|  2|PAWAN|            3.0|\n",
      "+---+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 'sai', 20),\n",
    "    (2, 'pawan', 30)\n",
    "]\n",
    "schema = ['id', 'name', 'salary in Thousnds']\n",
    "df =  spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import upper, col\n",
    "\n",
    "# So we need to transform the DataFrame such that name values are in uppercase and the salary are in lakhs and not in thousands.\n",
    "\n",
    "# Transform function will help to incorporate multiple tranforamtions under one function.\n",
    "def transfrom_data(df):\n",
    "    df = df.withColumn('name', upper('name'))\n",
    "    return df.withColumn('salary in Lakhs', (col('salary in Thousnds') * 10000) / 100000).drop('Salary in Thousnds')\n",
    "\n",
    "df.transform(transfrom_data).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id| name|           languages|\n",
      "+---+-----+--------------------+\n",
      "|  1|  sai|           [c, java]|\n",
      "|  2|pawan|[python, javascript]|\n",
      "+---+-----+--------------------+\n",
      "\n",
      "+---+-----+--------------------+--------------------+\n",
      "| id| name|           languages|   Langunages(Upper)|\n",
      "+---+-----+--------------------+--------------------+\n",
      "|  1|  sai|           [c, java]|           [C, JAVA]|\n",
      "|  2|pawan|[python, javascript]|[PYTHON, JAVASCRIPT]|\n",
      "+---+-----+--------------------+--------------------+\n",
      "\n",
      "+---+-----+--------------------+--------------------+\n",
      "| id| name|           languages|   Langunages(Upper)|\n",
      "+---+-----+--------------------+--------------------+\n",
      "|  1|  sai|           [c, java]|           [C, JAVA]|\n",
      "|  2|pawan|[python, javascript]|[PYTHON, JAVASCRIPT]|\n",
      "+---+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring another transfrom function.\n",
    "from pyspark.sql.functions import transform, upper, col\n",
    "# This transform will only work on Array Type columns.\n",
    "\n",
    "data = [\n",
    "    (1, 'sai', ['c', 'java']),\n",
    "    (2, 'pawan', ['python', 'javascript'])\n",
    "]\n",
    "schema = ['id', 'name', 'languages']\n",
    "df =  spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "# Using lambda function\n",
    "df.withColumn('Langunages(Upper)', transform('languages', lambda language: upper(language))).show()\n",
    "\n",
    "# Using normal function\n",
    "def upper_lan(language):\n",
    "    return upper(language)\n",
    "df.withColumn('Langunages(Upper)', transform('languages', upper_lan)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating or Replacing TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  sai|    20|\n",
      "|  2|pawan|    30|\n",
      "+---+-----+------+\n",
      "\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|  sai|    20|\n",
      "|pawan|    30|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 'sai', 20),\n",
    "    (2, 'pawan', 30)\n",
    "]\n",
    "schema = ['id', 'name', 'salary']\n",
    "df =  spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "\n",
    "# this will create a temporary view on the dataFrame which can be accessed only in that session.\n",
    "df.createOrReplaceTempView('employees')\n",
    "df1 = spark.sql('select name, salary from employees')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create or Replace global TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  sai|    20|\n",
      "|  2|pawan|    30|\n",
      "+---+-----+------+\n",
      "\n",
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|  sai|    20|\n",
      "|pawan|    30|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 22:30:36 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 'sai', 20),\n",
    "    (2, 'pawan', 30)\n",
    "]\n",
    "schema = ['id', 'name', 'salary']\n",
    "df =  spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "\n",
    "# this will create a view on the dataframe accross all the session.\n",
    "df.createOrReplaceGlobalTempView('emp')\n",
    "df1 = spark.sql('select name, salary from global_temp.emp')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(spark.catalog.currentDatabase())  # default database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='emp', catalog=None, namespace=['global_temp'], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='employees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('global_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop temp view\n",
    "# spark.catalog.dropTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, collect_list\n",
    "from pyspark.sql.types import IntegerType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+\n",
      "| Id| name|salary|bonus|\n",
      "+---+-----+------+-----+\n",
      "|  1|  sai|  2000| 1000|\n",
      "|  2|pawan|  3000|  500|\n",
      "+---+-----+------+-----+\n",
      "\n",
      "+---+-----+------+-----+---------+\n",
      "| Id| name|salary|bonus|total_pay|\n",
      "+---+-----+------+-----+---------+\n",
      "|  1|  sai|  2000| 1000|     3000|\n",
      "|  2|pawan|  3000|  500|     3500|\n",
      "+---+-----+------+-----+---------+\n",
      "\n",
      "+---+-----+------+-----+------------+\n",
      "| Id| name|salary|bonus|       array|\n",
      "+---+-----+------+-----+------------+\n",
      "|  1|  sai|  2000| 1000|[2000, 1000]|\n",
      "|  2|pawan|  3000|  500| [3000, 500]|\n",
      "+---+-----+------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 'sai', 2000, 1000),\n",
    "    (2, 'pawan', 3000, 500)\n",
    "]\n",
    "schema = ['Id', 'name', 'salary', 'bonus']\n",
    "df =  spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "# Creating a Function that will add salary and bonus, and name it as total pay\n",
    "def total_pay(salary:int, bonus:int)->int:\n",
    "    return salary + bonus\n",
    "total_payment = udf(total_pay, IntegerType())\n",
    "df.withColumn('total_pay', total_payment(df.salary, df.bonus)).show()\n",
    "\n",
    "# Using annotations\n",
    "# Creating a function to return a list of with elements as salary and bonus.\n",
    "@udf(ArrayType(elementType=IntegerType()))\n",
    "def get_array(*args):\n",
    "    return list(args)\n",
    "df.withColumn('array', get_array(df.salary, df.bonus)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|  sai|    20|\n",
      "|  2|pawan|    30|\n",
      "+---+-----+------+\n",
      "\n",
      "+---+-----+------+------+\n",
      "| id| name|salary|totPay|\n",
      "+---+-----+------+------+\n",
      "|  1|  sai|    20|  1020|\n",
      "|  2|pawan|    30|  1030|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registering the fucntion in registrey to access it along with sql command.\n",
    "spark.udf.register(name = 'total_payment', f = total_payment)\n",
    "spark.sql('SELECT * FROM employees').show()\n",
    "spark.sql('SELECT *, total_payment(salary, 1000) as totPay FROM employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
