{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/04 19:24:46 WARN Utils: Your hostname, Ds-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.29.89 instead (on interface en0)\n",
      "24/03/04 19:24:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/04 19:24:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark  = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure types and Structure fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "|_id|        Name|   Salary|\n",
      "+---+------------+---------+\n",
      "|  1|{sai, pawan}|    200.4|\n",
      "|  2|  {D, pawan}|2231324.0|\n",
      "+---+------------+---------+\n",
      "\n",
      "root\n",
      " |-- _id: integer (nullable = true)\n",
      " |-- Name: struct (nullable = true)\n",
      " |    |-- FirstName: string (nullable = true)\n",
      " |    |-- LastName: string (nullable = true)\n",
      " |-- Salary: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "name_structure = StructType([\n",
    "        StructField(name=\"FirstName\", dataType=StringType()),\n",
    "        StructField(name=\"LastName\", dataType=StringType())\n",
    "    ])\n",
    "\n",
    "data_structure = StructType([\n",
    "    StructField(name=\"_id\", dataType=IntegerType()),\n",
    "    StructField(name=\"Name\", dataType = name_structure),\n",
    "    StructField(name=\"Salary\", dataType=FloatType())\n",
    "])\n",
    "\n",
    "data = [\n",
    "    [1, ('sai', 'pawan'), 200.4],\n",
    "    [2, ('D', 'pawan'), 2231324.0]\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, data_structure)\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "|_id|      Scores|   Salary|\n",
      "+---+------------+---------+\n",
      "|  1|[20, 40, 50]|    200.4|\n",
      "|  2|    [10, 51]|2231324.0|\n",
      "+---+------------+---------+\n",
      "\n",
      "root\n",
      " |-- _id: integer (nullable = true)\n",
      " |-- Scores: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- Salary: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "data_structure = StructType([\n",
    "    StructField(name=\"_id\", dataType=IntegerType()),\n",
    "    StructField(name=\"Scores\", dataType=ArrayType(elementType=IntegerType())),\n",
    "    StructField(name=\"Salary\", dataType=FloatType())\n",
    "])\n",
    "\n",
    "data = [\n",
    "    [1, (20, 40, 50), 200.4],\n",
    "    [2, (10, 51), 2231324.0]\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, data_structure)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some operation on df using withColumn function for creating or experimenting with arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+-------------------+\n",
      "|_id|      Scores|   Salary|First Array element|\n",
      "+---+------------+---------+-------------------+\n",
      "|  1|[20, 40, 50]|    200.4|                 20|\n",
      "|  2|    [10, 51]|2231324.0|                 10|\n",
      "+---+------------+---------+-------------------+\n",
      "\n",
      "+---+------------+---------+-------------------+\n",
      "|_id|      Scores|   Salary|First Array element|\n",
      "+---+------------+---------+-------------------+\n",
      "|  1|[20, 40, 50]|    200.4|                 20|\n",
      "|  2|    [10, 51]|2231324.0|                 10|\n",
      "+---+------------+---------+-------------------+\n",
      "\n",
      "+---+------------+---------+-----------------+\n",
      "|_id|      Scores|   Salary|          numbers|\n",
      "+---+------------+---------+-----------------+\n",
      "|  1|[20, 40, 50]|    200.4|    [20.0, 200.4]|\n",
      "|  2|    [10, 51]|2231324.0|[10.0, 2231324.0]|\n",
      "+---+------------+---------+-----------------+\n",
      "\n",
      "+---+------------+---------+--------------------+\n",
      "|_id|      Scores|   Salary|             numbers|\n",
      "+---+------------+---------+--------------------+\n",
      "|  1|[20, 40, 50]|    200.4|[20.0, 40.0, 50.0...|\n",
      "|  2|    [10, 51]|2231324.0|[10.0, 51.0, 2231...|\n",
      "+---+------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, array, array_append\n",
    "\n",
    "# get the 1st element in above created array as a seperate column.\n",
    "\n",
    "df.withColumn('First Array element', col=df.Scores[0]).show()\n",
    "# This can also be done by\n",
    "df.withColumn('First Array element', col=col('Scores')[0]).show()\n",
    "\n",
    "# add the salary to the Socres column's 1st index and name the new column as Numbers\n",
    "df.withColumn(\"numbers\", col= array(col('Scores')[0], col('Salary'))).show()\n",
    "\n",
    "# add the salary to the Socres arry and name the new column as Numbers\n",
    "df.withColumn(\"numbers\", col= array_append(col('Scores'), col('Salary'))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring ArrayType Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+-----+\n",
      "|_id|      Scores|   Salary|Score|\n",
      "+---+------------+---------+-----+\n",
      "|  1|[20, 40, 50]|    200.4|   20|\n",
      "|  1|[20, 40, 50]|    200.4|   40|\n",
      "|  1|[20, 40, 50]|    200.4|   50|\n",
      "|  2|    [10, 51]|2231324.0|   10|\n",
      "|  2|    [10, 51]|2231324.0|   51|\n",
      "+---+------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode function\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Explode function will create new rows with the elements of the arrya specified in the function parameter.\n",
    "df.withColumn('Score', col=explode(col('Scores'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+\n",
      "|_id|     name|  salary|\n",
      "+---+---------+--------+\n",
      "|  1|sai,pawan|99314124|\n",
      "|  2|pawan,sai| 3268582|\n",
      "+---+---------+--------+\n",
      "\n",
      "+---+------------+--------+\n",
      "|_id|        name|  salary|\n",
      "+---+------------+--------+\n",
      "|  1|[sai, pawan]|99314124|\n",
      "|  2|[pawan, sai]| 3268582|\n",
      "+---+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split function\n",
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[1, 'sai,pawan', 99314124], [2, 'pawan,sai', 3268582]],\n",
    "    ['_id', 'name', 'salary']\n",
    ")\n",
    "\n",
    "df.show()\n",
    "\n",
    "# This will convert all the comma seperated names into array of names.\n",
    "df.withColumn('name', col=split(col('name'), ',')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+--------+\n",
      "|_id|FirstName|SecondName|  salary|\n",
      "+---+---------+----------+--------+\n",
      "|  1|      sai|     pawan|99314124|\n",
      "|  2|    pawan|       sai| 3268582|\n",
      "+---+---------+----------+--------+\n",
      "\n",
      "+---+---------+----------+--------+------------+\n",
      "|_id|FirstName|SecondName|  salary|        name|\n",
      "+---+---------+----------+--------+------------+\n",
      "|  1|      sai|     pawan|99314124|[sai, pawan]|\n",
      "|  2|    pawan|       sai| 3268582|[pawan, sai]|\n",
      "+---+---------+----------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array function\n",
    "from pyspark.sql.functions import array, col\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[1, 'sai', 'pawan', 99314124], [2, 'pawan', 'sai', 3268582]],\n",
    "    ['_id', 'FirstName', 'SecondName', 'salary']\n",
    ")\n",
    "\n",
    "df.show()\n",
    "\n",
    "# this function will combile two columns into a array. \n",
    "df.withColumn('name', col=array(col('FirstName'), col('SecondName'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+\n",
      "|_id|      Scores|   Salary|\n",
      "+---+------------+---------+\n",
      "|  1|[20, 40, 50]|    200.4|\n",
      "|  2|    [10, 51]|2231324.0|\n",
      "+---+------------+---------+\n",
      "\n",
      "+---+------------+---------+----------+\n",
      "|_id|      Scores|   Salary|vauleExist|\n",
      "+---+------------+---------+----------+\n",
      "|  1|[20, 40, 50]|    200.4|      true|\n",
      "|  2|    [10, 51]|2231324.0|     false|\n",
      "+---+------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Array contains function\n",
    "from pyspark.sql.functions import array_contains, col\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[1, [20, 40, 50], 200.4],[2, [10, 51, ], 2231324.0]], \n",
    "    ['_id', 'Scores', 'Salary']    \n",
    ")\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Return a boolean values representing if the values is there or now in the array.\n",
    "df.withColumn('vauleExist', col = array_contains(col('Scores'), 50)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Type columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining maptype columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------+\n",
      "|name |marks                       |\n",
      "+-----+----------------------------+\n",
      "|sai  |{Science -> 40, Maths -> 20}|\n",
      "|pawan|{Science -> 30, Maths -> 30}|\n",
      "+-----+----------------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- marks: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, MapType, StringType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "data = [['sai', {'Maths': 20, 'Science': 40}], ['pawan', {'Maths':30, 'Science': 30}]]\n",
    "\n",
    "# Defining schema to above data.\n",
    "schema = StructType([\n",
    "    StructField(name='name', dataType=StringType()),\n",
    "    StructField(name='marks', dataType=MapType(keyType=StringType(), valueType=IntegerType()))\n",
    "])\n",
    "\n",
    "df =  spark.createDataFrame(data, schema)\n",
    "\n",
    "# Truncate will dilplay all the information.\n",
    "df.show(truncate= False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the map data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+-----+\n",
      "| name|               marks|Science|Maths|\n",
      "+-----+--------------------+-------+-----+\n",
      "|  sai|{Science -> 40, M...|     40|   20|\n",
      "|pawan|{Science -> 30, M...|     30|   30|\n",
      "+-----+--------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Science', col = col('marks')['Science']).withColumn('Maths', col = col('marks')['Maths']).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring fuctions to deal with MapType data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "|name |key    |value|\n",
      "+-----+-------+-----+\n",
      "|sai  |Science|40   |\n",
      "|sai  |Maths  |20   |\n",
      "|pawan|Science|30   |\n",
      "|pawan|Maths  |30   |\n",
      "+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode Fucntion.\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select('name',explode(col=col('marks'))).show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------+----------------+--------+\n",
      "|name |marks                       |keys            |values  |\n",
      "+-----+----------------------------+----------------+--------+\n",
      "|sai  |{Science -> 40, Maths -> 20}|[Science, Maths]|[40, 20]|\n",
      "|pawan|{Science -> 30, Maths -> 30}|[Science, Maths]|[30, 30]|\n",
      "+-----+----------------------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Map keya and Map values\n",
    "from pyspark.sql.functions import map_keys, map_values\n",
    "\n",
    "df.withColumn('keys', map_keys(df.marks)).withColumn('values', map_values(df.marks)).show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
